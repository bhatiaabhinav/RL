\documentclass[11 pt]{article}
\usepackage{amsmath,amsfonts}
\usepackage{bm}
\usepackage[a4paper,margin=1in]{geometry}
\usepackage{xcolor}
\usepackage{pagecolor}
\usepackage{algorithm, algorithmicx, algpseudocode}

\definecolor{bgcolor}{RGB}{30,30,30}
\definecolor{fgcolor}{RGB}{225,225,225}
% \pagecolor{bgcolor}
% \color{fgcolor}

\begin{document}

\title{Optnet for DDPG Constrained $L_2$ Projection (QP)}
\author{Abhinav Bhatia}
\date{\today}
\maketitle


\section{Problem Statement}

Given a given node $g$ (which has already been allocated $C$ resources) from the constraints tree, we want to allocate resources to it's $k$ children such that:
\[\sum_i^k z_i = C \]
\[\forall i=1..k : 0 \le \check{C_i} \le z_i \le \hat{C_i} \le 1\]

We are given $C$ and a vector $\vec{y} \in [0, 1]^k$. We want to project this vector to the nearest (by $L_2$ norm) feasible solution $\vec{z}$.


\section{The Quadratic Program}

\begin{equation} \label{qp}
    \begin{aligned}
        \underset{\vec{z}}{\min} \sum_i^k (z_i - y_i)^2 \quad &\text{subject to} \\
        \sum_i^k z_i - C &= 0                   \qquad \lambda\\
        \forall i=1..k : z_i - \hat{C_i} &\le 0      \qquad \alpha_i\\
        \forall i=1..k : \check{C_i} - z_i &\le 0    \qquad \beta_i
    \end{aligned}
\end{equation}
Here $\lambda,\alpha_i,\beta_i$ are the corresponding Lagrange multipliers.

Since the objective function is \textit{strictly} convex and the constraints are convex too, there exists a unique solution to this QP.

The objective can be rewritten as:
\[f = \sum_i^k z_i^2 - \sum_i^k 2 y_i z_i + \sum_i^k y_i^2\]
The last term is a constant and is not really a part of the QP. Whether we include it or not, either way, it does not affect the KKT conditions.

If we want to write the objective function in form $\frac{1}{2}z^T Q z + c^Tz$, then $Q$ would be $2I$ and $\vec{c}$ would be $-2\vec{y}$.

\section{The KKT Conditions}

The Langragian is:
\begin{equation}
    L(\vec{z},\vec{\alpha},\vec{\beta},\lambda) = \sum_i^k (z_i-y_i)^2 + \lambda (\sum_i^k z_i - C) + \sum_i^k \alpha_i(z_i - \hat{C_i}) + \sum_i^k \beta_i(\check{C_i} - z_i)
\end{equation}

The KKT conditions (conditions satisfied by the solution $\vec{z}^*, \vec{\alpha}^*, \vec{\beta}^*, \lambda^*$ of the QP \ref{qp}) is given by
\begin{align*}
    &\nabla_{\vec{z},\lambda} L & = \vec{0} \\
    \forall i=1..k : \quad &\alpha_i(z_i - \hat{C_i}) & = 0 \\
    \forall i=1..k : \quad &\beta_i(\check{C_i} - z_i) & = 0
\end{align*}
which expand to:
\begin{equation}\label{eqns_kkt}
    \begin{cases}
        \begin{aligned}
            &\sum_i^k z_i - C & = 0 \\
            \forall i=1..k : \quad & 2(z_i-y_i) + \lambda + \alpha_i - \beta_i & = 0 \\
            \forall i=1..k : \quad & \alpha_i(z_i - \hat{C_i}) & = 0 \\
            \forall i=1..k : \quad & \beta_i(\check{C_i} - z_i) & = 0
        \end{aligned}
    \end{cases}
\end{equation}
i.e. $3k + 1$ equations.


\section{Differentiating the KKT conditions}

We can differentiate both sides of each equation in set of equations \ref{eqns_kkt} w.r.t to inputs $\vec{y}$ and $C$.

The partial differential equations w.r.t. input $y_j$ are:
\begin{equation}\label{eqns_wrt_y_j}
    \begin{cases}
        \begin{aligned}
            & \sum_i^k \frac{\partial z_i}{\partial y_j} & = 0 &\quad (a)\\
            
            \forall i=1..k : \quad & 2\frac{\partial z_i}{\partial y_j} - 2\delta_{ij} + \frac{\partial \lambda}{\partial y_j} + \frac{\partial \alpha_i}{\partial y_j} - \frac{\partial \beta_i}{\partial y_j} & = 0 &\quad (b)\\
            
            \forall i=1..k : \quad & \frac{\partial \alpha_i}{\partial y_j}(z_i - \hat{C_i}) + \alpha_i \frac{\partial z_i}{\partial y_j} & = 0 &\quad (c)\\
            
            \forall i=1..k : \quad & \frac{\partial \beta_i}{\partial y_j}(-z_i + \check{C_i}) - \beta_i \frac{\partial z_i}{\partial y_j}& = 0 &\quad (d)
        \end{aligned}
    \end{cases}
\end{equation}
Here $\delta_{ij}$ is the Kronecker delta function, which is $1$ when $i=j$, and $0$ otherwise.

The partial differential equations w.r.t $C$ are:
\begin{equation}\label{eqns_wrt_C}
    \begin{cases}
        \begin{aligned}
            & \sum_i^k \frac{\partial z_i}{\partial C} - 1 & = 0 &\quad (a)\\
            
            \forall i=1..k : \quad & 2\frac{\partial z_i}{\partial C} + \frac{\partial \lambda}{\partial C} + \frac{\partial \alpha_i}{\partial C} - \frac{\partial \beta_i}{\partial C} & = 0 &\quad (b) \\
            
            \forall i=1..k : \quad & \frac{\partial \alpha_i}{\partial C}(z_i - \hat{C_i}) + \alpha_i \frac{\partial z_i}{\partial C} & = 0 &\quad (c) \\
            
            \forall i=1..k : \quad & \frac{\partial \beta_i}{\partial C}(-z_i + \check{C_i}) - \beta_i \frac{\partial z_i}{\partial C}& = 0 &\quad (d)
        \end{aligned}
    \end{cases}
\end{equation}

The equations can be solved independently per input $y_j$ and $C$.


\section{Solving system of equations \ref{eqns_wrt_y_j} and \ref{eqns_wrt_C}} \label{section_solving_eqns}

For equations \ref{eqns_wrt_y_j}, the variables are $\frac{\partial}{\partial y_j}$ of $\alpha_i,\beta_i,\lambda,z_i$. So there are $n=3k + 1$ variables and that many equations.
Trying to write equations \ref{eqns_wrt_y_j} in matrix form:
\[A_{n \times n}^{y_j}J_{n \times 1}^{y_j} = B_{n \times 1}^{y_j}\]
where
\[J^{y_j} = \frac{\partial}{\partial y_j}[\lambda,\alpha_1,\beta_1,z_1,...,\alpha_k,\beta_k,z_k]^T\]
and
\[
A^{y_j} = \begin{bmatrix}
    eqn \ref{eqns_wrt_y_j} & \vdots & \lambda & \alpha_1 & \beta_1 & z_1 & \cdots & \alpha_i & \beta_i & z_i \\
    \cdots & \vdots & \hdotsfor{8} \\
    a & \vdots & 0 & 0 & 0 & 1 & \cdots & 0 & 0 & 1 \\
    b_1 & \vdots & 1 & 1 & -1 & 2 & \cdots & 0 & 0 & 0 \\
    c_1 & \vdots & 0 & z_1-\hat{C_1} & 0 & \alpha_1 & \cdots & 0 & 0 & 0 \\
    d_1 & \vdots & 0 & 0 & -z_1+\check{C_1} & -\beta_1 & \cdots & 0 & 0 & 0 \\
    \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots \\
    b_k & \vdots & 1 & 0 & 0 & 0 & \cdots & 1 & -1 & 2 \\
    c_k & \vdots & 0 & 0 & 0 & 0 & \cdots & z_k-\hat{C_k} & 0 & \alpha_k \\
    d_k & \vdots & 0 & 0 & 0 & 0 & \cdots & 0 & -z_k+\check{C_k} & -\beta_k \\
\end{bmatrix}
\]
and
\[
B^{y_j} = \begin{bmatrix}
    0 \\
    2\delta_{1j} \\
    0 \\
    0 \\
    \vdots \\
    2\delta_{kj} \\
    0 \\
    0 \\
\end{bmatrix}
\]
Basically,
\begin{equation} \label{eqn_A_yj}
    A_{rc}^{y_j} = \begin{cases}
        \begin{aligned}
            1 &\quad    r=1,c=3m+1 & m=1..k \\
            1 &\quad    r=3m-1,c=1 & m=1..k \\
            1 &\quad    r=3m-1,c=r & m=1..k \\
            -1 &\quad   r=3m-1,c=r+1 & m=1..k \\
            2 &\quad    r=3m-1,c=r+2 & m=1..k \\
            z_m-\hat{C_m} &\quad    r=3m,c=r-1 & m=1..k \\
            \alpha_m &\quad r=3m,c=r+1 & m=1..k \\
            -z_m+\check{C_m} &\quad  r=3m+1,c=r-1 & m=1..k \\
            -\beta_m &\quad r=3m+1,c=r & m=1..k \\
            0 &\quad \text{otherwise}
        \end{aligned}
    \end{cases}
\end{equation}
and
\begin{equation} \label{eqn_B_yj}
    B_r^{y_j} = \begin{cases}
        \begin{aligned}
            2 \delta_{mj} &\quad r=3m-1 & m=1..k \\
            0 &\quad \text{otherwise}
        \end{aligned}
    \end{cases}
\end{equation}
Thus we can find
\begin{equation}
    J^{y_j} = (A^{y_j})^{-1} B^{y_j}
\end{equation}

Note from equation \ref{eqn_A_yj} that $A^{y_j}$ does not depend on $j$. Thus its inverse need not be computed seperately for each and every $j$.

Also, It is clear from set of equations \ref{eqns_wrt_y_j} and \ref{eqns_wrt_C} that
\begin{equation}\label{eqn_all_A_same}
    \forall_j^k:\quad A^C = A^{y_j}
\end{equation}
Only $B^C$ is different:
\begin{equation}
    B_r^C = \begin{cases}
        \begin{aligned}
            1 &\quad r=1 \\
            0 &\quad \text{otherwise}
        \end{aligned}
    \end{cases}
\end{equation}
Thus,
\begin{equation}
    J^{C} = (A^C)^{-1} B^{C}
\end{equation}

The overall Jacobian would be simply the horizontal contatenation:
\begin{equation}
    J_{(3k+1)\times (k+1)} = \begin{bmatrix}J^{y_1}&J^{y_2}&...&J^{y_k}&J^C\end{bmatrix}
\end{equation}


\section{Issues}

There can be two main issues:
\begin{enumerate}
    \item Lots of hills and saddle points \\
            This can happen when for some $i$, $z_i = \hat{C_i}$ or $z_i = \check{C_i}$, irrespective of the exact value of $\vec{y}$. In that case, $\frac{\partial z_i}{\partial y_j} = 0, \forall j$, which means that irrespective of the loss function, $z_i$, would not change at this $y$. But this is not problem in itself, since $y$ can continue to change through $\frac{\partial z_{\ne i}}{\partial y_j}$, and for some other value of $y$, $z_i$ will start changing. But still, the whole thing can get stuck in a local minimum, if the gradient of the loss function becomes zero w.r.t all components of y. \\
            In practice, it does not get stuck in any local minimums, but this is really an issue of a landscape in which one has to go \textit{around} lots of hills, making the convergence slow. A hack is to make $\vec{y}$ have a range between $\vec{\check{C}}$ and $\vec{\hat{C}}$ by say preprocessing it with a scaled $tanh$ layer. But this $tanh$ layer can cause gradient vanishing gradients. A hack is to scale it to between $\vec{\check{C}} - \epsilon$ and $\vec{\hat{C}} + \epsilon$. This way the gradients due to $tanh$ become better; and although the original issue can appear again, it becomes way less severe (i.e. there are no long distances to travel around hills).
    \item Non differentiable points \\
            Happens when matrix $A$ has a zero row. This is possible if both $z_i - \hat{C_i} = \alpha_i = 0$ for some $i$. This would happen if the $y_i \le \hat{C_i}$ (making $\alpha=0$) and gets projected to the boundary ($z_i = \hat{C_i}$). An example would be that $y$ was a feasible point and atleast one of the components was \textit{exactly on} the boundary. In that case, $z=y$, and for those boundary components, $z_i - \hat{C_i} = \alpha_i = 0$.
    \item Small gradients \\
            Overall, the gradients through this layer are typically very small. This is easily fixed by having a larger learning rate (~10x).
\end{enumerate}


\section{Batched Problem}

Now we want to solve the problem for the entire batch simultaneously. Let the batch size be $N$.

For sample $s$, given node $g_s$ (which has already been allocated $C_s$ resources) from the constraints tree, we want to allocate resources to it's $k$ children such that:
\[\sum_i^k z_{si} = C_s \]
\[\forall i \in {1,...,k} : 0 \le \check{C_{si}} \le z_{si} \le \hat{C_{si}} \le 1\]

i.e. for all $s \in {1,...,N}$, we are given $C_s$ and a vector $\vec{y_s} \in [0, 1]^k$. We want to project this vector to the nearest (by $L_2$ norm) feasible solution $\vec{z_s}$.


\section{Batched QP}

The objective of the combined QP should be just the sum of objectives of the individual QPs.

\begin{equation} \label{batch_qp}
    \begin{aligned}
        \underset{\bm{z}}{\min} \sum_s^N \sum_i^k (z_{si} - y_{si})^2 \quad &\text{subject to} \\
        \forall s=1..N: & \sum_i^k z_{si} - C_s &= 0                   \qquad \lambda_s\\
        \forall s=1..N: \forall i=1..k : & \ z_{si} - \hat{C_{si}} &\le 0      \qquad \alpha_{si}\\
        \forall s=1..N: \forall i=1..k : & \ -z_{si} + \check{C_{si}} &\le 0      \qquad \beta_{si}\\
    \end{aligned}
\end{equation}
Here $\lambda_s,\alpha_{si}, \beta_{si}$ are the corresponding Lagrange multipliers.

Since the objective function is \textit{strictly} convex and the constraints are convex too, there exists a unique solution to this QP.

The objective can be rewritten as:
\[f = \sum_s^N \sum_i^k z_{si}^2 - \sum_s^N \sum_i^k 2 y_{si} z_{si} + \sum_s^N \sum_i^k y_{si}^2\]
The last term is a constant and is not really a part of the QP. Whether we include it or not, either way, it does not affect the KKT conditions.

For inputs to programs like cplex, if we want to write the objective function in form $\frac{1}{2}z^T Q z + c^Tz$, (here $z$ is a vector of $Nk$ dimensions, i.e. flattened version of $\bm{z}$) then $Q$ would be $2I$ and $\vec{c}$ would be $-2\vec{y}$ (here $\vec{y}$ is flattened version of $\bm{y}$)


\section{Batched QP KKT Conditions}

The Langragian is:
\begin{equation}
    % L(\vec{z},\vec{\alpha},\vec{\beta},\lambda) = \sum_i^k (z_i-y_i)^2 + \lambda (\sum_i^k z_i - C) + \sum_i^k \alpha_i(z_i - \hat{C_i}) + \sum_i^k \beta_i(\check{C_i} - z_i)
    L(\bm{z},\bm{\alpha},\bm{\beta},\vec{\lambda}) = \sum_s^N \sum_i^k (z_{si}-y_{si})^2 + \sum_s^N \lambda_s (\sum_i^k z_{si} - C_s) + \sum_s^N \sum_i^k \alpha_{si}(z_{si} - \hat{C_{si}}) + \sum_s^N \sum_i^k \beta_{si}(\check{C_{si}} - z_{si})
\end{equation}

The KKT conditions (conditions satisfied by the solution $\bm{z}^*, \bm{\alpha}^*, \bm{\beta}^* \vec{\lambda}^*$ of the QP \ref{batch_qp}) is given by
\begin{align*}
    &\nabla_{\bm{z},\vec{\lambda}} L & = \bm{0} \\
    \forall s=1..N: \forall i=1..k: \quad &\alpha_{si}(z_{si} - \hat{C_{si}}) & = 0 \\
    \forall s=1..N: \forall i=1..k: \quad &\beta_{si}(-z_{si} + \check{C_{si}}) & = 0 \\
\end{align*}
which expand to:
\begin{equation}\label{eqns_batch_kkt}
    \begin{cases}
        \begin{aligned}
            \forall s=1..N &: \sum_i^k z_{si} - C_s & = 0 \\
            \forall s=1..N : \forall i=1..k \quad &: 2(z_{si}-y_{si}) + \lambda_s + \alpha_{si} + \beta_{si} & = 0 \\
            \forall s=1..N : \forall i=1..k \quad &: \alpha_{si}(z_{si} - \hat{C_{si}}) & = 0 \\
            \forall s=1..N : \forall i=1..k \quad &: \beta_{si}(-z_{si} + \check{C_{si}}) & = 0 \\
        \end{aligned}
    \end{cases}
\end{equation}
i.e. $N(3k+1)$ equations.


\section{Differentiating the Batch KKT conditions}

We can differentiate both sides of each equation in set of equations \ref{eqns_batch_kkt} w.r.t to inputs $\bm{y}$ and $\vec{C}$.

\textbf{We already know that the Jacobian of $\vec{z_s}$ w.r.t $\vec{y_t}$ will be zero when $t \ne s$ , since samples do not affect each other during the forward pass. Thus we need to compute individual Jacobians only.}

Let the subscript $s$ be implicit in all the text from this point. i.e. we are talking about sample $s$.

The partial differential equations are exactly same as \ref{eqns_wrt_y_j} and \ref{eqns_wrt_C}, which can be solved independently per input $y_j$ and $C$.

\section{Solving system of equations \ref{eqns_wrt_y_j} and \ref{eqns_wrt_C}}

Exact same procedure as in section \ref{section_solving_eqns} can be followed to calculate the Jacobian per sample $s=1..N$.


\section{Closed form solution}
TODO


\section{Computation requirements}

Computation needed to compute $A^{-1}$ will be of order $k^3$. And luckily it need not be computed for all $y_j$ and $C$ (by \ref{eqn_all_A_same}). $A^{-1}B$ ($\propto k^2$) will be done $k$ times, i.e. $k^3$.
Thus just $\mathcal{O}(k^3)$ per backward pass.

For the entire minibatch, $\mathcal{O}(N k^3)$. Computation per minibatch can be done parallely. Plus all the matrix operations can be parallelized on a GPU. So the complexity can be dialed back to less than $\mathcal{O}(k^3)$.


\section{Implementation}

We can define new ops in tensorflow. for backward pass, the matrix calculations can be done using the tensorflow function, i.e. it can be done on a GPU.


\section{2-level constraints}

Let us try to consider 2-level constraints. Notation:

\begin{center}
    \begin{tabular}{r l}
        $N$ & batch size i.e. number of samples \\
        $n$ & refering to sameple $n$ \\
        $K$ & total number of zones \\
        $k$ & refering to zone $k$ \\
        $G$ & total number of groups/regions \\
        $g$ & refering to group g \\
        $\mathbb{K}_g$ & set of zone ids under $g$. Mutually exclusive and exhaustive. Thus $\sum_{g=1}^G |\mathbb{K}_g| = K$ \\
        $gr(k)$ & = $g$ s.t. $k \in \mathbb{K}_g$ \\
        $z_{nk}$ & allocation for zone $k$ for sample $n$ \\
        $y_{nk}$ & neural network output for zone $k$ for sample $n$\\
        $z_k$ & allocation for zone $k$ ($n$ implicit from context)\\
        $y_k$ & neural network output for zone $k$ ($n$ implicit from context)\\
        $c$ & sum of resources. typically $c=1$ \\
        $\hat{c_k}$ & upper bound contraint for $z_k$ \\
        $\check{c_k}$ & lower bound contraint for $z_k$ \\
        $\hat{m_g}$ & upper bound contraint for $\sum_{k \in \mathbb{K}_g} z_k$ \\
        $\check{m_g}$ & lower bound contraint for $\sum_{k \in \mathbb{K}_g} z_k$  \\
    \end{tabular}
\end{center}

\subsection{QP}

\begin{equation} \label{batch_nested_qp}
    \begin{aligned}
        \underset{\bm{z}}{\min} \sum_n^N \sum_k^K (z_{nk} - y_{nk})^2 \quad &\text{subject to} \\
        \forall n=1..N: & \sum_k^K z_{nk} - c &= 0                   \qquad \lambda_n\\
        \forall n=1..N: \forall g=1..G: & \sum_{k \in \mathbb{K}_g} z_{nk} - \hat{m_{g}} &\le 0 \qquad \mu_{ng} \\
        \forall n=1..N: \forall g=1..G: & -\sum_{k \in \mathbb{K}_g} z_{nk} + \check{m_{g}} &\le 0 \qquad \nu_{ng} \\
        \forall n=1..N: \forall k=1..K: & \ z_{nk} - \hat{c_{k}} &\le 0      \qquad \alpha_{nk}\\
        \forall n=1..N: \forall k=1..K : & \ -z_{nk} + \check{c_{k}} &\le 0 \qquad \beta_{nk}\\
    \end{aligned}
\end{equation}
Here $\lambda_n, \mu_{ng}, \nu_{ng}, \alpha_{nk}, \beta_{nk}$ are the corresponding Lagrange multipliers.


\subsection{KKT Conditions}

The Langragian is:
\begin{multline}
    L(\bm{z},\bm{\alpha},\bm{\beta},\bm{\mu},\bm{\nu},\vec{\lambda}) = \sum_n^N \sum_k^K (z_{nk}-y_{nk})^2 + \sum_n^N \lambda_n (\sum_k^K z_{nk} - c) \\
    + \sum_n^N \sum_g^G \mu_{ng}(\sum_{k \in \mathbb{K}_g} z_{nk} - \hat{m_{g}})
    + \sum_n^N \sum_g^G \nu_{ng}(-\sum_{k \in \mathbb{K}_g} z_{nk} + \check{m_{g}}) \\
    + \sum_n^N \sum_k^K \alpha_{nk}(z_{nk} - \hat{c_k}) + \sum_n^N \sum_k^K \beta_{nk}(\check{c_k} - z_{nk})
\end{multline}

The KKT conditions (conditions satisfied by the solution $\bm{z}^*, \bm{\mu}^*, \bm{\nu}^*, \bm{\alpha}^*, \bm{\beta}^* \vec{\lambda}^*$ of the QP \ref{batch_nested_qp}) is given by
\begin{align*}
    &\nabla_{\bm{z},\vec{\lambda}} L & = \bm{0} \\
    \forall n=1..N: \forall g=1..G: \quad &\mu_{ng}(\sum_{k \in \mathbb{K}_g} z_{nk} - \hat{m_{g}}) & = 0 \\
    \forall n=1..N: \forall g=1..G: \quad &\nu_{ng}(-\sum_{k \in \mathbb{K}_g} z_{nk} + \check{m_{g}}) & = 0 \\
    \forall n=1..N: \forall k=1..K: \quad &\alpha_{nk}(z_{nk} - \hat{c_k}) & = 0 \\
    \forall n=1..N: \forall k=1..K: \quad &\beta_{nk}(-z_{nk} + \check{c_k}) & = 0 \\
\end{align*}
which expand to:
\begin{equation}\label{eqns_batch_nesteted_kkt}
    \begin{cases}
        \begin{aligned}
            \forall n=1..N &: \sum_k^K z_{nk} - c & = 0 \\
            \forall n=1..N : \forall k=1..K \quad &: 2(z_{nk}-y_{nk}) + \lambda_n + \mu_{n,gr(k)} - \nu_{n,gr(k)} + \alpha_{nk} + \beta_{nk} & = 0 \\
            \forall n=1..N: \forall g=1..G: \quad &\mu_{ng}(\sum_{k \in \mathbb{K}_g} z_{nk} - \hat{m_{g}}) & = 0 \\
            \forall n=1..N: \forall g=1..G: \quad &\nu_{ng}(-\sum_{k \in \mathbb{K}_g} z_{nk} + \check{m_{g}}) & = 0 \\
            \forall n=1..N: \forall k=1..K: \quad &\alpha_{nk}(z_{nk} - \hat{c_k}) & = 0 \\
            \forall n=1..N: \forall k=1..K: \quad &\beta_{nk}(-z_{nk} + \check{c_k}) & = 0 \\
        \end{aligned}
    \end{cases}
\end{equation}
i.e. $N(3K + 2G + 1)$ equations.


\subsection{Differenting KKT equantions}
Since, samples do not affect each other, $\frac{\partial z_{n_1 k_1}}{\partial y_{n_2 k_2}}=0$ when $n_1 \ne n_2$. So the KKT conditions can be differentiated independently for each $n$. Let the subscript $n$ be implicit from now on.

The partial differential equations w.r.t. input $y_j$ are:
\begin{equation}\label{eqns_nested_wrt_y_j}
    \begin{cases}
        \begin{aligned}
            & \sum_k^K \frac{\partial z_k}{\partial y_j} & = 0 &\quad (a)\\
            
            \forall k=1..K : \quad & 2\frac{\partial z_i}{\partial y_j} - 2\delta_{kj} + \frac{\partial \lambda}{\partial y_j} + \frac{\partial \mu_{gr(k)}}{\partial y_j} - \frac{\partial \nu_{gr(k)}}{\partial y_j} + \frac{\partial \alpha_k}{\partial y_j} - \frac{\partial \beta_k}{\partial y_j} & = 0 &\quad (b)\\
            
            \forall k=1..K : \quad & \frac{\partial \alpha_k}{\partial y_j}(z_k - \hat{c_k}) + \alpha_k \frac{\partial z_k}{\partial y_j} & = 0 &\quad (c)\\
            
            \forall k=1..K : \quad & \frac{\partial \beta_k}{\partial y_j}(z_k - \check{c_k}) + \beta_k \frac{\partial z_k}{\partial y_j}& = 0 &\quad (d) \\

            \forall g=1..G : \quad & \frac{\partial \mu_g}{\partial y_j}(\sum_{k \in \mathbb{K}_g} z_{k} - \hat{m_{g}}) + \mu_g \sum_{k \in \mathbb{K}_g} \frac{\partial z_k}{\partial y_j}& = 0 &\quad (e) \\

            \forall g=1..G : \quad & \frac{\partial \nu_g}{\partial y_j}(\sum_{k \in \mathbb{K}_g} z_{k} - \check{m_{g}}) + \nu_g \sum_{k \in \mathbb{K}_g} \frac{\partial z_k}{\partial y_j}& = 0 &\quad (f)
        \end{aligned}
    \end{cases}
\end{equation}
Here $\delta_{kj}$ is the Kronecker delta function, which is $1$ when $k=j$, and $0$ otherwise.


\section{Approximate Optnet}
The QP \ref{qp} can be solved approximately using the following algorithm:

\begin{algorithm} \label{algo_apprx_qp}
    \caption{Approximate Solution to QP \ref{qp}}
    \begin{algorithmic}[1]
        \Require $k \ge 2$
        \Require $\forall i=1..k: 0 \le \check{c_i} < \hat{c_i} \le c$
        \Require $\sum \check{c_i} < c < \sum \hat{c_i}$
        \State $k' \gets k$
        \State $c' \gets c$
        \State $\Omega \gets \{1,2,...,k\}$ \Comment{unassigned output indices}
        \State $\text{phase} \gets \text{LOWER}$ \Comment{possible values are: $\text{LOWER} = 0; \text{UPPER}=1; \text{DONE}=2$}
        \While{$\text{phase} \ne \text{DONE}$}
            \State $\Omega' \gets \phi$
            \For{$i \in \Omega$}
                \State $z_i \gets y_i + (c'-\sum_{j \in \Omega} y_j)/k'$
                \State $J_{ij} \gets \delta_{ij} - 1/k' \ \mathbf{foreach}\  j \in \Omega'$
                \For{$j \in \Omega$}
                    
                \EndFor
                \If{$z_i < \check{c_i}\  \textbf{and}\  \text{phase}=\text{LOWER}$}
                    \State $z_i \gets \check{c_i}$
                    \State $J_{ij} \gets 0 \ \mathbf{foreach}\  j=1..k$
                    \State $\Omega' \gets \Omega' \cup \{i\}$
                \ElsIf{$z_i > \hat{c_i} \ \textbf{and} \  \text{phase}=\text{UPPER}$}
                    \State $z_i \gets \hat{c_i}$
                    \State $J_{ij} \gets 0 \ \mathbf{foreach}\  j=1..k$
                    \State $\Omega' \gets \Omega' \cup \{i\}$
                \EndIf
            \EndFor

            \State $k' \gets k'- |\Omega'|$
            \State $c' \gets c' - \sum_{i \in \Omega'} z_i$
            \State $\Omega \gets \Omega - \Omega'$
            \If{$\Omega' = \phi$}
                \State $\text{phase} = \text{phase} + 1$
            \EndIf
        \EndWhile
        \State \Return $z, J$
        \Ensure $\forall i: \check{c_i} \le z_i \le \hat{c_i}$
        \Ensure $\sum_{i=1}^k z_i = c$
        \Ensure $z = y \ \mathbf{if}\ \sum_{i=1}^k y_i = c \ \mathbf{and}\ \forall i: \check{c_i} \le y_i \le \hat{c_i}$
    \end{algorithmic}
\end{algorithm}


\end{document}