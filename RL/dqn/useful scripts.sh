python -m RL.dqn.dqn --env_id=PongNoFrameskip-v4 --experiment_name=4StepSmallDQN --convs=[(16,8,4),(32,4,2)] --states_embedding_hidden_layers=[] --Q_hidden_layers=[256] --n_episodes=10000 --experience_buffer_length=50000 --minimum_experience=5000 --final_epsilon=0.1 --eval_epsilon=0.05 --epsilon_anneal_over=50000 --nsteps=4 --render=True --render_interval=8 --render_vsync=True --video_interval=8 --save_every=8

python -m RL.dqn.dqn --env_id=PongNoFrameskip-v4 --experiment_name=4StepSmallDQN_Test --eval_mode=True --convs=[(16,8,4),(32,4,2)] --states_embedding_hidden_layers=[] --Q_hidden_layers=[256] --n_episodes=100 --eval_epsilon=0.05 --render=True --render_interval=4 --render_vsync=True --video_interval=4 --load_model_dir=D:\Abhinav\workspace\logs\PongNoFrameskip-v4\4StepSmallDQN\save_models

python -m RL.dqn.dqn --env_id=PongNoFrameskip-v4 --experiment_name=4StepDQN --convs=[(32,8,4),(64,4,2),(64,3,1)] --states_embedding_hidden_layers=[] --Q_hidden_layers=[512] --n_episodes=10000 --experience_buffer_length=50000 --minimum_experience=5000 --final_epsilon=0.1 --eval_epsilon=0.05 --epsilon_anneal_over=50000 --nsteps=4 --render=True --render_interval=8 --render_vsync=True --video_interval=8 --save_every=8

python -m RL.dqn.dqn --env_id=PongNoFrameskip-v4 --experiment_name=4StepSmallDDDQN --convs=[(16,8,4),(32,4,2)] --states_embedding_hidden_layers=[] --Q_hidden_layers=[256] --double_dqn=True --dueling_dqn=True --n_episodes=10000 --experience_buffer_length=50000 --minimum_experience=5000 --final_epsilon=0.1 --eval_epsilon=0.05 --epsilon_anneal_over=50000 --nsteps=4 --render=True --render_interval=8 --render_vsync=True --video_interval=8 --save_every=8

# safe DQN:
python -m RL.dqn.safe_dqn --env_id=PongNoFrameskip-v4 --experiment_name=-0.5Safe_4Step_Small_DQN --safety_threshold=-0.5 --convs=[(16,8,4),(32,4,2)] --states_embedding_hidden_layers=[] --Q_hidden_layers=[256] --n_episodes=10000 --experience_buffer_length=50000 --minimum_experience=5000 --final_epsilon=0.1 --eval_epsilon=0.05 --epsilon_anneal_over=50000 --nsteps=4 --render=True --render_interval=8 --render_vsync=True --video_interval=8 --save_every=8

python -m RL.dqn.safe_dqn --env_id=PongNoFrameskip-v4 --experiment_name=-5e-1Safe_4Step_Small_DQN --safety_threshold=-0.5 --convs=[(16,8,4),(32,4,2)] --states_embedding_hidden_layers=[] --Q_hidden_layers=[256] --n_episodes=10000 --experience_buffer_length=50000 --minimum_experience=5000 --final_epsilon=0.1 --eval_epsilon=0.05 --epsilon_anneal_over=50000 --nsteps=4 --render=True --render_interval=8 --render_vsync=True --video_interval=8 --save_every=8

python -m RL.dqn.safe_dqn --env_id=PongNoFrameskip-v4 --experiment_name=25PenaltySafe_4Step_Small_DQN --penalty_safe_dqn_multiplier=25 --convs=[(16,8,4),(32,4,2)] --states_embedding_hidden_layers=[] --Q_hidden_layers=[256] --n_episodes=10000 --experience_buffer_length=50000 --minimum_experience=5000 --final_epsilon=0.1 --eval_epsilon=0.05 --epsilon_anneal_over=50000 --nsteps=4 --render=True --render_interval=8 --render_vsync=True --video_interval=8 --save_every=8


# to run on server: safe dqn:
xvfb-run -a python -m RL.dqn.safe_dqn --env_id=PongNoFrameskip-v4 --experiment_name=-5e-1Safe4StepDQN --eval_mode=False --n_episodes=1000 --double_dqn=False --penalty_safe_dqn_mode=False --safety_threshold=-0.5 --nsteps=4 --convs="[(32,8,4),(64,4,2),(64,3,1)]" --states_embedding_hidden_layers=[] --Q_hidden_layers=[512] --learning_rate=1e-4 --minibatch_size=32 --train_every=4 --experience_buffer_length=100000 --minimum_experience=10000 --final_epsilon=0.1 --eval_epsilon=0.05 --epsilon_anneal_over=100000 --ddpg_target_network_update_mode=True --tau=0.001 --exploit_every=4 --video_interval=8 --save_every=8

# to run on server: safe dqn:
xvfb-run -a python -m RL.dqn.safe_dqn --env_id=PongNoFrameskip-v4 --experiment_name=-0.1Safe4StepDDQN --eval_mode=False --n_episodes=1000 --double_dqn=True --penalty_safe_dqn_mode=False --safety_threshold=-0.1 --nsteps=4 --convs="[(32,8,4),(64,4,2),(64,3,1)]" --states_embedding_hidden_layers=[] --Q_hidden_layers=[512] --gamma=0.99 --learning_rate=6.25e-5 --minibatch_size=32 --train_every=4 --experience_buffer_length=200000 --minimum_experience=20000 --final_epsilon=0.01 --eval_epsilon=0.001 --epsilon_anneal_over=100000 --ddpg_target_network_update_mode=True --tau=0.001 --exploit_every=4 --video_interval=8 --save_every=32

# eval:
xvfb-run -a python -m RL.dqn.safe_dqn --env_id=PongNoFrameskip-v4 --experiment_name=-0.1Safe4StepDDQN_eval --eval_mode=True --n_episodes=100 --double_dqn=True --penalty_safe_dqn_mode=False --safety_threshold=-0.1 --nsteps=4 --convs="[(32,8,4),(64,4,2),(64,3,1)]" --states_embedding_hidden_layers=[] --Q_hidden_layers=[512] --gamma=0.99 --learning_rate=6.25e-5 --minibatch_size=32 --train_every=4 --experience_buffer_length=200000 --minimum_experience=20000 --final_epsilon=0.01 --eval_epsilon=0.001 --epsilon_anneal_over=100000 --ddpg_target_network_update_mode=True --tau=0.001 --exploit_every=4 --video_interval=1 --save_every=32 --render=True --load_model_dir="$OPENAI_LOGDIR/PongNoFrameskip-v4/-0.1Safe4StepDDQN/saved_models"

# on windows:
xvfb-run -a python -m RL.dqn.safe_dqn --env_id=PongNoFrameskip-v4 --experiment_name=-0.0Safe4StepDDQN_cont --eval_mode=False --n_episodes=1000 --double_dqn=True --penalty_safe_dqn_mode=False --safety_threshold=-0.0 --nsteps=4 --convs="[(32,8,4),(64,4,2),(64,3,1)]" --states_embedding_hidden_layers=[] --Q_hidden_layers=[512] --gamma=0.99 --learning_rate=6.25e-5 --minibatch_size=32 --train_every=4 --experience_buffer_length=50000 --minimum_experience=20000 --final_epsilon=0.01 --eval_epsilon=0.001 --epsilon_anneal_over=100000 --ddpg_target_network_update_mode=True --tau=0.001 --exploit_every=4 --video_interval=8 --save_every=32 --render=False --load_model_dir="D:\Abhinav\workspace\logs\PongNoFrameskip-v4\-0.1Safe4StepDDQN\saved_models"

# to run on server: penalty dqn (final params):
xvfb-run -a  -s "-screen 0 1024x768x24 -ac +extension GLX +render -noreset" python -m RL.dqn.safe_dqn --env_id=PongNoFrameskip-v4 --experiment_name=25PenaltySafe_3StepDDQN --eval_mode=False --n_episodes=1000 --double_dqn=True --penalty_safe_dqn_mode=True --penalty_safe_dqn_multiplier=25 --atari_clip_rewards=True --atari_framestack_k=4 --atari_episode_life=True --gamma=0.99 --nsteps=3 --convs="[(32,8,4),(64,4,2),(64,3,1)]" --states_embedding_hidden_layers=[] --Q_hidden_layers=[512] --learning_rate=6.25e-5 --minibatch_size=32 --train_every=4 --target_network_update_every=4 --target_network_update_tau=0.001 --experience_buffer_length=200000 --minimum_experience=20000 --final_epsilon=0.01 --eval_epsilon=0.001 --epsilon_anneal_over=100000 --exploit_every=4 --video_interval=10 --save_every=100

# to run on server: safety dqn (final params):
xvfb-run -a  -s "-screen 0 1024x768x24 -ac +extension GLX +render -noreset" python -m RL.dqn.safe_dqn --env_id=PongNoFrameskip-v4 --experiment_name=-0.1AdvancedSafe_3StepDDQN --eval_mode=False --n_episodes=1000 --double_dqn=True --penalty_safe_dqn_mode=False --safety_threshold=-0.1 --atari_clip_rewards=True --atari_framestack_k=4 --atari_episode_life=True --gamma=0.99 --nsteps=3 --convs="[(32,8,4),(64,4,2),(64,3,1)]" --states_embedding_hidden_layers=[] --Q_hidden_layers=[512] --learning_rate=6.25e-5 --minibatch_size=32 --train_every=4 --target_network_update_every=4 --target_network_update_tau=0.001 --experience_buffer_length=200000 --minimum_experience=20000 --final_epsilon=0.01 --eval_epsilon=0.001 --epsilon_anneal_over=100000 --exploit_every=4 --video_interval=10 --save_every=100

# to run on server: normal dqn (final params):
xvfb-run -a  -s "-screen 0 1024x768x24 -ac +extension GLX +render -noreset" python -m RL.dqn.safe_dqn --env_id=PongNoFrameskip-v4 --experiment_name=3StepDDQN --eval_mode=False --n_episodes=1000 --double_dqn=True --penalty_safe_dqn_mode=True --penalty_safe_dqn_multiplier=0 --atari_clip_rewards=True --atari_framestack_k=4 --atari_episode_life=True --gamma=0.99 --nsteps=3 --convs="[(32,8,4),(64,4,2),(64,3,1)]" --states_embedding_hidden_layers=[] --Q_hidden_layers=[512] --learning_rate=6.25e-5 --minibatch_size=32 --train_every=4 --target_network_update_every=4 --target_network_update_tau=0.001 --experience_buffer_length=200000 --minimum_experience=20000 --final_epsilon=0.01 --eval_epsilon=0.001 --epsilon_anneal_over=100000 --exploit_every=4 --video_interval=10 --save_every=100

xvfb-run -a python -m RL.dqn.dqn --env_id=PongNoFrameskip-v4 --experiment_name=4StepDQN --eval_mode=False --n_episodes=1000 --double_dqn=False --nsteps=4 --convs="[(32,8,4),(64,4,2),(64,3,1)]" --states_embedding_hidden_layers=[] --Q_hidden_layers=[512] --learning_rate=1e-4 --minibatch_size=32 --train_every=4 --experience_buffer_length=100000 --minimum_experience=10000 --final_epsilon=0.1 --eval_epsilon=0.05 --epsilon_anneal_over=100000 --ddpg_target_network_update_mode=True --tau=0.001 --exploit_every=4 --video_interval=8 --save_every=8


# --------------------------------------------- LunarLander-v2 Scripts ------------------------------------------------

# normal dqn:
xvfb-run -a -s "-screen 0 1024x768x24 -ac +extension GLX +render -noreset" python -m RL.dqn.safe_dqn --env_id=LunarLander-v2 --lunar_no_crash_penalty_main_stream=False --experiment_name=3StepDDQN --eval_mode=False --n_episodes=1500 --double_dqn=True --penalty_safe_dqn_mode=True --penalty_safe_dqn_multiplier=0 --gamma=0.99 --nsteps=3 --states_embedding_hidden_layers=[] --Q_hidden_layers=[400,300] --learning_rate=6.25e-5 --minibatch_size=32 --train_every=4 --target_network_update_every=4 --target_network_update_tau=0.001 --experience_buffer_length=200000 --minimum_experience=20000 --final_epsilon=0.01 --eval_epsilon=0.001 --epsilon_anneal_over=100000 --exploit_every=4 --video_interval=50 --save_every=100

# no crash penalty (in main stream) and normal dqn:
xvfb-run -a -s "-screen 0 1024x768x24 -ac +extension GLX +render -noreset" python -m RL.dqn.safe_dqn --env_id=LunarLander-v2 --lunar_no_crash_penalty_main_stream=True --experiment_name=NCPMS_3StepDDQN --eval_mode=False --n_episodes=1500 --double_dqn=True --penalty_safe_dqn_mode=True --penalty_safe_dqn_multiplier=0 --gamma=0.99 --nsteps=3 --states_embedding_hidden_layers=[] --Q_hidden_layers=[400,300] --learning_rate=6.25e-5 --minibatch_size=32 --train_every=4 --target_network_update_every=4 --target_network_update_tau=0.001 --experience_buffer_length=200000 --minimum_experience=20000 --final_epsilon=0.01 --eval_epsilon=0.001 --epsilon_anneal_over=100000 --exploit_every=4 --video_interval=50 --save_every=100

# no crash penalty (in main stream) and 100penalty-safe dqn:
xvfb-run -a -s "-screen 0 1024x768x24 -ac +extension GLX +render -noreset" python -m RL.dqn.safe_dqn --env_id=LunarLander-v2 --lunar_no_crash_penalty_main_stream=True --experiment_name=NCPMS_100PSafe_3StepDDQN --eval_mode=False --n_episodes=1500 --double_dqn=True --penalty_safe_dqn_mode=True --penalty_safe_dqn_multiplier=100 --gamma=0.99 --nsteps=3 --states_embedding_hidden_layers=[] --Q_hidden_layers=[400,300] --learning_rate=6.25e-5 --minibatch_size=32 --train_every=4 --target_network_update_every=4 --target_network_update_tau=0.001 --experience_buffer_length=200000 --minimum_experience=20000 --final_epsilon=0.01 --eval_epsilon=0.001 --epsilon_anneal_over=100000 --exploit_every=4 --video_interval=50 --save_every=100

# no crash penalty (in main stream) and safe dqn:
xvfb-run -a -s "-screen 0 1024x768x24 -ac +extension GLX +render -noreset" python -m RL.dqn.safe_dqn --env_id=LunarLander-v2 --lunar_no_crash_penalty_main_stream=True --experiment_name=NCPMS_-0.1AdvancedSafe_3StepDDQN --eval_mode=False --n_episodes=2000 --double_dqn=True --penalty_safe_dqn_mode=False --safety_threshold=-0.1 --gamma=0.99 --nsteps=3 --states_embedding_hidden_layers=[] --Q_hidden_layers=[400,300] --learning_rate=6.25e-5 --minibatch_size=32 --train_every=4 --target_network_update_every=4 --target_network_update_tau=0.001 --experience_buffer_length=200000 --minimum_experience=20000 --final_epsilon=0.01 --eval_epsilon=0.001 --epsilon_anneal_over=100000 --exploit_every=4 --video_interval=50 --save_every=100 --plot_Q=True --render_interval=50